"""
Evaluation and Feedback Service for Advanced RAG System.
This module implements comprehensive evaluation metrics, golden dataset management,
and continuous learning feedback loops for improving RAG performance.
"""

import logging
import json
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
import hashlib
import statistics
from dataclasses import dataclass

from ..core.db_mongo import get_mongodb_client, MongoDBClient
from ..core.cache_redis import get_redis_client, RedisClient
from ..config import settings

logger = logging.getLogger(__name__)

@dataclass
class EvaluationMetrics:
    """Container for evaluation metrics."""
    retrieval_precision: float
    retrieval_recall: float
    answer_accuracy: float
    response_relevance: float
    hallucination_rate: float
    context_utilization: float
    latency: float
    token_efficiency: float
    user_satisfaction: float

class EvaluationService:
    """Service for evaluating and improving RAG system performance."""
    
    def __init__(self):
        self.mongo_client = None
        self.redis_client = None
        self.golden_dataset = None
    
    async def initialize(self):
        """Initialize the evaluation service."""
        try:
            logger.info("Initializing evaluation service...")
            
            # Get database clients
            self.mongo_client = await get_mongodb_client()
            self.redis_client = await get_redis_client()
            
            # Only initialize MongoDB-dependent features if MongoDB is available
            if self.mongo_client.database is not None:
                # Load golden dataset
                await self._load_golden_dataset()
                
                # Initialize evaluation collections
                await self._ensure_evaluation_collections()
                logger.info("Evaluation service initialized with MongoDB support")
            else:
                logger.warning("MongoDB not available, evaluation service running with limited functionality")
            
            logger.info("Evaluation service initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize evaluation service: {e}")
            # Don't raise - allow the service to continue with limited functionality
    
    async def evaluate_query_response(self, query: str, response: str, sources: List[Dict],
                                    processing_time: float, context_used: str,
                                    session_id: str) -> EvaluationMetrics:
        """
        Evaluate a single query-response pair across multiple metrics.
        
        Args:
            query: User query
            response: Generated response
            sources: Retrieved sources
            processing_time: Time taken to process
            context_used: Context provided to LLM
            session_id: Session identifier
        
        Returns:
            EvaluationMetrics object
        """
        try:
            # Calculate retrieval metrics
            retrieval_precision = self._calculate_retrieval_precision(query, sources)
            retrieval_recall = self._calculate_retrieval_recall(query, sources)
            
            # Calculate answer quality metrics
            answer_accuracy = await self._calculate_answer_accuracy(query, response)
            response_relevance = self._calculate_response_relevance(query, response)
            
            # Calculate reliability metrics
            hallucination_rate = self._calculate_hallucination_rate(response, context_used)
            context_utilization = self._calculate_context_utilization(response, context_used)
            
            # Calculate efficiency metrics
            token_efficiency = self._calculate_token_efficiency(response, context_used)
            
            # Get user satisfaction (if available)
            user_satisfaction = await self._get_user_satisfaction(session_id)
            
            metrics = EvaluationMetrics(
                retrieval_precision=retrieval_precision,
                retrieval_recall=retrieval_recall,
                answer_accuracy=answer_accuracy,
                response_relevance=response_relevance,
                hallucination_rate=hallucination_rate,
                context_utilization=context_utilization,
                latency=processing_time,
                token_efficiency=token_efficiency,
                user_satisfaction=user_satisfaction
            )
            
            # Store evaluation results
            await self._store_evaluation_result(
                query, response, sources, metrics, session_id
            )
            
            return metrics
            
        except Exception as e:
            logger.error(f"Query evaluation failed: {e}")
            # Return default metrics
            return EvaluationMetrics(
                retrieval_precision=0.0,
                retrieval_recall=0.0,
                answer_accuracy=0.0,
                response_relevance=0.0,
                hallucination_rate=1.0,
                context_utilization=0.0,
                latency=processing_time,
                token_efficiency=0.0,
                user_satisfaction=0.0
            )
    
    async def record_user_feedback(self, session_id: str, message_id: str, 
                                 feedback_type: str, rating: Optional[int] = None,
                                 correction: Optional[str] = None,
                                 comment: Optional[str] = None) -> bool:
        """
        Record user feedback for continuous learning.
        
        Args:
            session_id: Session identifier
            message_id: Specific message being rated
            feedback_type: Type of feedback (rating, correction, etc.)
            rating: Numerical rating (1-5)
            correction: Corrected response text
            comment: Additional user comments
        
        Returns:
            Success status
        """
        try:
            feedback_data = {
                "session_id": session_id,
                "message_id": message_id,
                "feedback_type": feedback_type,
                "rating": rating,
                "correction": correction,
                "comment": comment,
                "timestamp": datetime.utcnow(),
                "processed": False
            }
            
            # Store feedback in database
            result = await self.mongo_client.db.user_feedback.insert_one(feedback_data)
            
            # Update real-time satisfaction cache
            if rating is not None:
                await self._update_satisfaction_cache(session_id, rating)
            
            # Queue for learning pipeline if correction provided
            if correction:
                await self._queue_for_learning(session_id, message_id, correction)
            
            logger.info(f"Recorded feedback: {feedback_type} for session {session_id}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to record feedback: {e}")
            return False
    
    async def get_system_performance_report(self, days: int = 7) -> Dict[str, Any]:
        """
        Generate a comprehensive system performance report.
        
        Args:
            days: Number of days to analyze
        
        Returns:
            Performance report dictionary
        """
        try:
            start_date = datetime.utcnow() - timedelta(days=days)
            
            # Query evaluation data
            evaluation_data = await self.mongo_client.db.evaluations.find({
                "timestamp": {"$gte": start_date}
            }).to_list(None)
            
            if not evaluation_data:
                return self._empty_report()
            
            # Calculate aggregate metrics
            metrics = self._calculate_aggregate_metrics(evaluation_data)
            
            # Get trending data
            trends = self._calculate_performance_trends(evaluation_data, days)
            
            # Get user feedback summary
            feedback_summary = await self._get_feedback_summary(start_date)
            
            # Get top performing queries
            top_queries = await self._get_top_performing_queries(start_date)
            
            # Get problematic patterns
            issues = await self._identify_performance_issues(evaluation_data)
            
            return {
                "period": f"Last {days} days",
                "total_queries": len(evaluation_data),
                "average_metrics": metrics,
                "trends": trends,
                "user_feedback": feedback_summary,
                "top_queries": top_queries,
                "identified_issues": issues,
                "recommendations": self._generate_recommendations(metrics, issues),
                "generated_at": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to generate performance report: {e}")
            return self._empty_report()
    
    async def run_golden_dataset_evaluation(self) -> Dict[str, Any]:
        """
        Run evaluation against the golden dataset.
        
        Returns:
            Golden dataset evaluation results
        """
        try:
            if not self.golden_dataset:
                return {"error": "No golden dataset available"}
            
            results = []
            
            for item in self.golden_dataset:
                query = item["query"]
                expected_answer = item["expected_answer"]
                expected_sources = item.get("expected_sources", [])
                
                # This would normally call the chat service to get actual response
                # For now, we'll simulate the evaluation structure
                
                evaluation_result = {
                    "query": query,
                    "expected_answer": expected_answer,
                    "expected_sources": expected_sources,
                    "status": "pending",  # Would be "passed" or "failed" after actual evaluation
                    "accuracy_score": 0.0,
                    "retrieval_score": 0.0
                }
                
                results.append(evaluation_result)
            
            # Calculate overall performance
            overall_score = sum(r.get("accuracy_score", 0) for r in results) / len(results)
            
            return {
                "total_tests": len(results),
                "overall_score": overall_score,
                "results": results,
                "evaluated_at": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Golden dataset evaluation failed: {e}")
            return {"error": str(e)}
    
    async def _load_golden_dataset(self):
        """Load the golden dataset for evaluation."""
        try:
            if not self.mongo_client.database:
                logger.warning("MongoDB not available, using default golden dataset")
                self.golden_dataset = self._get_default_golden_dataset()
                return
                
            # Try to load from database first
            golden_data = await self.mongo_client.db.golden_dataset.find().to_list(None)
            
            if golden_data:
                self.golden_dataset = golden_data
                logger.info(f"Loaded {len(golden_data)} items from golden dataset")
            else:
                # Create default golden dataset
                await self._create_default_golden_dataset()
                
        except Exception as e:
            logger.warning(f"Failed to load golden dataset: {e}")
            self.golden_dataset = []
    
    async def _create_default_golden_dataset(self):
        """Create a default golden dataset for evaluation."""
        
        default_dataset = [
            {
                "query": "What is the company's return policy?",
                "expected_answer": "Items can be returned within 30 days of purchase with receipt.",
                "expected_sources": ["policy_document_1"],
                "category": "policy",
                "difficulty": "easy"
            },
            {
                "query": "How do I reset my password?",
                "expected_answer": "Click on 'Forgot Password' on the login page and follow email instructions.",
                "expected_sources": ["help_documentation"],
                "category": "technical",
                "difficulty": "easy"
            },
            {
                "query": "What are the system requirements for the software?",
                "expected_answer": "Windows 10 or later, 8GB RAM, 2GB free disk space, internet connection.",
                "expected_sources": ["system_requirements"],
                "category": "technical", 
                "difficulty": "medium"
            },
            {
                "query": "Explain the data privacy and security measures",
                "expected_answer": "We use industry-standard encryption, regular security audits, and comply with GDPR regulations.",
                "expected_sources": ["privacy_policy", "security_documentation"],
                "category": "compliance",
                "difficulty": "hard"
            }
        ]
        
        # Store in database
        await self.mongo_client.db.golden_dataset.insert_many(default_dataset)
        self.golden_dataset = default_dataset
        
        logger.info("Created default golden dataset with 4 test cases")
    
    def _calculate_retrieval_precision(self, query: str, sources: List[Dict]) -> float:
        """Calculate precision of retrieved sources."""
        if not sources:
            return 0.0
        
        # Simple heuristic: sources with higher scores are considered more relevant
        relevant_count = sum(1 for source in sources if source.get("score", 0) > 0.7)
        return relevant_count / len(sources)
    
    def _calculate_retrieval_recall(self, query: str, sources: List[Dict]) -> float:
        """Calculate recall of retrieved sources."""
        # This would need comparison with known relevant documents
        # For now, return a heuristic based on source diversity
        
        if not sources:
            return 0.0
        
        # Check source diversity (different document types/sources)
        unique_sources = len(set(source.get("doc_id", "") for source in sources))
        max_expected_sources = 5  # Assume max 5 relevant sources per query
        
        return min(unique_sources / max_expected_sources, 1.0)
    
    async def _calculate_answer_accuracy(self, query: str, response: str) -> float:
        """Calculate accuracy of the generated answer."""
        
        # Check for golden dataset match
        if self.golden_dataset:
            for item in self.golden_dataset:
                if self._queries_similar(query, item["query"]):
                    expected = item["expected_answer"].lower()
                    actual = response.lower()
                    return self._text_similarity(expected, actual)
        
        # Fallback: basic quality indicators
        quality_score = 0.0
        
        # Check if response is substantial
        if len(response.split()) > 5:
            quality_score += 0.3
        
        # Check for coherence (no repetition)
        words = response.split()
        unique_words = len(set(words))
        if unique_words / len(words) > 0.7:
            quality_score += 0.3
        
        # Check for factual language patterns
        factual_patterns = ["according to", "based on", "the document states", "as mentioned"]
        if any(pattern in response.lower() for pattern in factual_patterns):
            quality_score += 0.4
        
        return min(quality_score, 1.0)
    
    def _calculate_response_relevance(self, query: str, response: str) -> float:
        """Calculate relevance of response to query."""
        
        query_words = set(query.lower().split())
        response_words = set(response.lower().split())
        
        # Calculate word overlap
        overlap = len(query_words.intersection(response_words))
        max_overlap = min(len(query_words), len(response_words))
        
        if max_overlap == 0:
            return 0.0
        
        overlap_score = overlap / max_overlap
        
        # Boost score if response addresses query type
        query_type_words = {
            "what": ["is", "are", "definition", "meaning"],
            "how": ["steps", "process", "method", "way"],
            "why": ["because", "reason", "cause", "due"],
            "when": ["time", "date", "schedule", "timing"],
            "where": ["location", "place", "address", "site"]
        }
        
        for q_word, response_indicators in query_type_words.items():
            if q_word in query.lower():
                if any(indicator in response.lower() for indicator in response_indicators):
                    overlap_score += 0.2
                break
        
        return min(overlap_score, 1.0)
    
    def _calculate_hallucination_rate(self, response: str, context: str) -> float:
        """Estimate hallucination rate in the response."""
        
        if not context:
            return 1.0  # No context means potential hallucination
        
        # Check if response content is grounded in context
        response_sentences = response.split('.')
        context_words = set(context.lower().split())
        
        grounded_sentences = 0
        
        for sentence in response_sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:
                continue
                
            sentence_words = set(sentence.lower().split())
            # If more than 30% of words in sentence appear in context, consider grounded
            overlap = len(sentence_words.intersection(context_words))
            if overlap / len(sentence_words) > 0.3:
                grounded_sentences += 1
        
        total_sentences = len([s for s in response_sentences if len(s.strip()) > 10])
        
        if total_sentences == 0:
            return 0.0
        
        grounding_rate = grounded_sentences / total_sentences
        hallucination_rate = 1.0 - grounding_rate
        
        return max(0.0, min(1.0, hallucination_rate))
    
    def _calculate_context_utilization(self, response: str, context: str) -> float:
        """Calculate how well the response utilizes the provided context."""
        
        if not context:
            return 0.0
        
        context_words = set(context.lower().split())
        response_words = set(response.lower().split())
        
        # Calculate what portion of context words appear in response
        utilized_words = len(context_words.intersection(response_words))
        utilization_rate = utilized_words / len(context_words) if context_words else 0.0
        
        return min(utilization_rate, 1.0)
    
    def _calculate_token_efficiency(self, response: str, context: str) -> float:
        """Calculate token efficiency (response quality per token)."""
        
        response_length = len(response.split())
        context_length = len(context.split())
        
        if response_length == 0:
            return 0.0
        
        # Efficiency = useful response length / total tokens processed
        total_tokens = response_length + context_length
        efficiency = response_length / total_tokens if total_tokens > 0 else 0.0
        
        # Bonus for concise but complete responses
        if 50 <= response_length <= 200:
            efficiency *= 1.2
        
        return min(efficiency, 1.0)
    
    async def _get_user_satisfaction(self, session_id: str) -> float:
        """Get user satisfaction score for the session."""
        
        try:
            # Get recent feedback for this session
            feedback = await self.mongo_client.db.user_feedback.find({
                "session_id": session_id,
                "rating": {"$exists": True}
            }).sort("timestamp", -1).limit(5).to_list(None)
            
            if not feedback:
                return 0.5  # Neutral score when no feedback
            
            ratings = [f["rating"] for f in feedback if f.get("rating")]
            if ratings:
                return sum(ratings) / len(ratings) / 5.0  # Normalize to 0-1
            
            return 0.5
            
        except Exception as e:
            logger.warning(f"Failed to get user satisfaction: {e}")
            return 0.5
    
    async def _store_evaluation_result(self, query: str, response: str, sources: List[Dict],
                                     metrics: EvaluationMetrics, session_id: str):
        """Store evaluation results in database."""
        
        try:
            evaluation_doc = {
                "query": query,
                "response": response,
                "sources": sources,
                "session_id": session_id,
                "metrics": {
                    "retrieval_precision": metrics.retrieval_precision,
                    "retrieval_recall": metrics.retrieval_recall,
                    "answer_accuracy": metrics.answer_accuracy,
                    "response_relevance": metrics.response_relevance,
                    "hallucination_rate": metrics.hallucination_rate,
                    "context_utilization": metrics.context_utilization,
                    "latency": metrics.latency,
                    "token_efficiency": metrics.token_efficiency,
                    "user_satisfaction": metrics.user_satisfaction
                },
                "timestamp": datetime.utcnow()
            }
            
            await self.mongo_client.db.evaluations.insert_one(evaluation_doc)
            
        except Exception as e:
            logger.error(f"Failed to store evaluation result: {e}")
    
    async def _ensure_evaluation_collections(self):
        """Ensure evaluation database collections exist."""
        if not self.mongo_client.database:
            logger.warning("MongoDB not available, skipping collection creation")
            return
            
        collections = ["evaluations", "user_feedback", "golden_dataset", "performance_issues"]
        
        existing_collections = await self.mongo_client.db.list_collection_names()
        
        for collection in collections:
            if collection not in existing_collections:
                await self.mongo_client.db.create_collection(collection)
                logger.info(f"Created evaluation collection: {collection}")
    
    def _queries_similar(self, query1: str, query2: str) -> bool:
        """Check if two queries are similar."""
        return self._text_similarity(query1.lower(), query2.lower()) > 0.7
    
    def _text_similarity(self, text1: str, text2: str) -> float:
        """Calculate simple text similarity."""
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 and not words2:
            return 1.0
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def _calculate_aggregate_metrics(self, evaluation_data: List[Dict]) -> Dict[str, float]:
        """Calculate aggregate metrics from evaluation data."""
        
        if not evaluation_data:
            return {}
        
        metrics = {}
        metric_names = [
            "retrieval_precision", "retrieval_recall", "answer_accuracy",
            "response_relevance", "hallucination_rate", "context_utilization",
            "latency", "token_efficiency", "user_satisfaction"
        ]
        
        for metric_name in metric_names:
            values = []
            for eval_data in evaluation_data:
                metric_value = eval_data.get("metrics", {}).get(metric_name)
                if metric_value is not None:
                    values.append(metric_value)
            
            if values:
                metrics[metric_name] = {
                    "mean": statistics.mean(values),
                    "median": statistics.median(values),
                    "std_dev": statistics.stdev(values) if len(values) > 1 else 0.0,
                    "min": min(values),
                    "max": max(values)
                }
        
        return metrics
    
    def _calculate_performance_trends(self, evaluation_data: List[Dict], days: int) -> Dict[str, Any]:
        """Calculate performance trends over time."""
        
        # Group data by day
        daily_data = {}
        
        for eval_data in evaluation_data:
            timestamp = eval_data.get("timestamp", datetime.utcnow())
            day_key = timestamp.strftime("%Y-%m-%d")
            
            if day_key not in daily_data:
                daily_data[day_key] = []
            
            daily_data[day_key].append(eval_data)
        
        # Calculate daily averages
        trends = {}
        
        for day, day_evaluations in daily_data.items():
            day_metrics = self._calculate_aggregate_metrics(day_evaluations)
            
            trends[day] = {
                "query_count": len(day_evaluations),
                "avg_accuracy": day_metrics.get("answer_accuracy", {}).get("mean", 0.0),
                "avg_latency": day_metrics.get("latency", {}).get("mean", 0.0),
                "avg_satisfaction": day_metrics.get("user_satisfaction", {}).get("mean", 0.0)
            }
        
        return trends
    
    async def _get_feedback_summary(self, start_date: datetime) -> Dict[str, Any]:
        """Get summary of user feedback."""
        
        try:
            feedback_data = await self.mongo_client.db.user_feedback.find({
                "timestamp": {"$gte": start_date}
            }).to_list(None)
            
            if not feedback_data:
                return {"total_feedback": 0}
            
            # Calculate feedback statistics
            ratings = [f["rating"] for f in feedback_data if f.get("rating")]
            corrections = [f for f in feedback_data if f.get("correction")]
            comments = [f for f in feedback_data if f.get("comment")]
            
            return {
                "total_feedback": len(feedback_data),
                "rating_count": len(ratings),
                "avg_rating": statistics.mean(ratings) if ratings else 0.0,
                "correction_count": len(corrections),
                "comment_count": len(comments),
                "rating_distribution": self._calculate_rating_distribution(ratings)
            }
            
        except Exception as e:
            logger.error(f"Failed to get feedback summary: {e}")
            return {"total_feedback": 0}
    
    def _calculate_rating_distribution(self, ratings: List[int]) -> Dict[str, int]:
        """Calculate distribution of ratings."""
        
        distribution = {str(i): 0 for i in range(1, 6)}
        
        for rating in ratings:
            if 1 <= rating <= 5:
                distribution[str(rating)] += 1
        
        return distribution
    
    async def _get_top_performing_queries(self, start_date: datetime) -> List[Dict[str, Any]]:
        """Get top performing queries."""
        
        try:
            # Find queries with high accuracy scores
            top_queries = await self.mongo_client.db.evaluations.find({
                "timestamp": {"$gte": start_date}
            }).sort("metrics.answer_accuracy", -1).limit(10).to_list(None)
            
            return [
                {
                    "query": q["query"],
                    "accuracy": q["metrics"]["answer_accuracy"],
                    "user_satisfaction": q["metrics"]["user_satisfaction"]
                }
                for q in top_queries
            ]
            
        except Exception as e:
            logger.error(f"Failed to get top performing queries: {e}")
            return []
    
    async def _identify_performance_issues(self, evaluation_data: List[Dict]) -> List[Dict[str, Any]]:
        """Identify performance issues from evaluation data."""
        
        issues = []
        
        if not evaluation_data:
            return issues
        
        # High hallucination rate
        high_hallucination = [e for e in evaluation_data 
                            if e.get("metrics", {}).get("hallucination_rate", 0) > 0.3]
        
        if len(high_hallucination) > len(evaluation_data) * 0.1:
            issues.append({
                "type": "high_hallucination",
                "severity": "medium",
                "description": f"{len(high_hallucination)} queries had high hallucination rates",
                "affected_queries": len(high_hallucination)
            })
        
        # Low accuracy
        low_accuracy = [e for e in evaluation_data 
                       if e.get("metrics", {}).get("answer_accuracy", 0) < 0.5]
        
        if len(low_accuracy) > len(evaluation_data) * 0.2:
            issues.append({
                "type": "low_accuracy",
                "severity": "high",
                "description": f"{len(low_accuracy)} queries had low accuracy scores",
                "affected_queries": len(low_accuracy)
            })
        
        # High latency
        high_latency = [e for e in evaluation_data 
                       if e.get("metrics", {}).get("latency", 0) > 5.0]
        
        if len(high_latency) > len(evaluation_data) * 0.1:
            issues.append({
                "type": "high_latency", 
                "severity": "low",
                "description": f"{len(high_latency)} queries had high response times",
                "affected_queries": len(high_latency)
            })
        
        return issues
    
    def _generate_recommendations(self, metrics: Dict[str, Any], issues: List[Dict]) -> List[str]:
        """Generate recommendations based on metrics and issues."""
        
        recommendations = []
        
        # Check for specific issues
        for issue in issues:
            if issue["type"] == "high_hallucination":
                recommendations.append(
                    "Consider improving context compression and source filtering to reduce hallucinations"
                )
            elif issue["type"] == "low_accuracy":
                recommendations.append(
                    "Review and expand the knowledge base, or improve chunk relevance scoring"
                )
            elif issue["type"] == "high_latency":
                recommendations.append(
                    "Optimize retrieval pipeline or consider caching frequently asked queries"
                )
        
        # General recommendations based on metrics
        if not metrics:
            recommendations.append("Insufficient data for specific recommendations")
        else:
            avg_accuracy = metrics.get("answer_accuracy", {}).get("mean", 0.0)
            if avg_accuracy < 0.7:
                recommendations.append(
                    "Consider fine-tuning the LLM or improving prompt engineering"
                )
            
            avg_satisfaction = metrics.get("user_satisfaction", {}).get("mean", 0.0) 
            if avg_satisfaction < 0.6:
                recommendations.append(
                    "Focus on improving response quality and user experience"
                )
        
        return recommendations if recommendations else ["System performance appears satisfactory"]
    
    def _empty_report(self) -> Dict[str, Any]:
        """Return empty report structure."""
        return {
            "period": "No data available",
            "total_queries": 0,
            "average_metrics": {},
            "trends": {},
            "user_feedback": {"total_feedback": 0},
            "top_queries": [],
            "identified_issues": [],
            "recommendations": ["Insufficient data for analysis"],
            "generated_at": datetime.utcnow().isoformat()
        }
    
    async def _update_satisfaction_cache(self, session_id: str, rating: int):
        """Update satisfaction cache for real-time metrics."""
        
        try:
            cache_key = f"satisfaction:{session_id}"
            
            # Get current ratings for session
            cached_data = await self.redis_client.get_json(cache_key)
            
            if cached_data:
                ratings = cached_data.get("ratings", [])
                ratings.append(rating)
                ratings = ratings[-10:]  # Keep only last 10 ratings
            else:
                ratings = [rating]
            
            # Update cache
            await self.redis_client.set_json(
                cache_key, 
                {"ratings": ratings, "avg": sum(ratings) / len(ratings)},
                expire_seconds=86400  # 24 hours
            )
            
        except Exception as e:
            logger.warning(f"Failed to update satisfaction cache: {e}")
    
    async def _queue_for_learning(self, session_id: str, message_id: str, correction: str):
        """Queue correction for learning pipeline."""
        
        try:
            learning_data = {
                "session_id": session_id,
                "message_id": message_id,
                "correction": correction,
                "queued_at": datetime.utcnow(),
                "processed": False
            }
            
            await self.mongo_client.db.learning_queue.insert_one(learning_data)
            
            logger.info(f"Queued correction for learning: {message_id}")
            
        except Exception as e:
            logger.error(f"Failed to queue for learning: {e}")

# Global instance
evaluation_service = EvaluationService()